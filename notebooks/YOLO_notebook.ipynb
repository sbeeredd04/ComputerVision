{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model\n",
    "\n",
    "- Make sure to use \n",
    "    - ```pip install ultralytics```\n",
    "    - ```pip install opencv-python```\n",
    "    - ```pip install mediapipe```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#loading the model\n",
    "model = YOLO('yolov8s.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Recognition in a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# Load and process image\n",
    "img = cv2.imread('IMG_0954.jpg')\n",
    "results = model(img)[0]\n",
    "\n",
    "# Save results using the correct method\n",
    "# Option 1: Using YOLO's save method\n",
    "results.save(filename='output.jpg')\n",
    "\n",
    "# Option 2: Using OpenCV with plotted results\n",
    "plotted_img = results.plot()\n",
    "cv2.imwrite('output.jpg', plotted_img)\n",
    "\n",
    "# Clean exit\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if you want a different box\n",
    "\n",
    "#### Syntax for OpenCV: \n",
    "##### Rectangle syntax\n",
    "- cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "- cv2.rectangle(img, (x1, y1), (x2, y2), (B, G, R), 2)\n",
    "\n",
    "##### Text syntax\n",
    "- cv2.putText(image, text, org, font, fontScale, color, thickness)\n",
    "- cv2.putText(img, \"Hello\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (B, G, R), 2)\n",
    "\n",
    "##### Common parameters:\n",
    "- Colors: (B,G,R)\n",
    "  - Blue: (255, 0, 0)\n",
    "  - Green: (0, 255, 0)\n",
    "  - Red: (0, 0, 255)\n",
    "  - Black: (0, 0, 0)\n",
    "  - White: (255, 255, 255)\n",
    "\n",
    "- Fonts:\n",
    "  - cv2.FONT_HERSHEY_SIMPLEX \n",
    "  - cv2.FONT_HERSHEY_PLAIN\n",
    "  - cv2.FONT_HERSHEY_DUPLEX\n",
    "  - cv2.FONT_HERSHEY_COMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#loading the image\n",
    "img = cv2.imread('IMG_0954.jpg')\n",
    "\n",
    "results = model(img)[0]\n",
    "\n",
    "for result in results.boxes.data.tolist():\n",
    "    \n",
    "    x1, y1, x2, y2, score, class_id = result\n",
    "    class_name = results.names[int(class_id)]\n",
    "    \n",
    "    print(f\"Detected {class_name} with confidence {score:.2f} at [{x1}, {y1}, {x2}, {y2}]\")\n",
    "    \n",
    "    #drawing the bounding box\n",
    "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 0), 2)\n",
    "    cv2.putText(img, f\"{class_name} {score:.2f}\", (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 6)\n",
    "    \n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imwrite('output.jpg', img)\n",
    "\n",
    "#stop everything on q press\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if you want Video Capture ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#using webcam (VIDEO CAPTURE)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#(OPTIONAL) setting the resolution\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "#loading the model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    results = model(frame)[0]\n",
    "    \n",
    "    for result in results.boxes.data.tolist():\n",
    "        \n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "        class_name = results.names[int(class_id)]\n",
    "        \n",
    "        print(f\"Detected {class_name} with confidence {score:.2f} at [{x1}, {y1}, {x2}, {y2}]\")\n",
    "        \n",
    "        #drawing the bounding box\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 0), 2)\n",
    "        cv2.putText(frame, f\"{class_name} {score:.2f}\", (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 6)\n",
    "        \n",
    "    cv2.imshow('image', frame)\n",
    "    \n",
    "    #stop everything on q press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## Did you know you can track hands ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#loading the model\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "# Initialize Mediapipe Hand Detection\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Webcam input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks,\n",
    "                                   mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Hand Skeleton Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about gestures then ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task\n",
    "\n",
    "!curl -O https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7)\n",
    "\n",
    "# Gesture recognizer setup\n",
    "model_path = 'gesture_recognizer.task'\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Global variable for gesture results\n",
    "gesture_texts = []\n",
    "\n",
    "def print_result(result, output_image, timestamp_ms):\n",
    "    global gesture_texts\n",
    "    gesture_texts = []\n",
    "    if result.gestures and result.handedness:\n",
    "        for hand, gesture in zip(result.handedness, result.gestures):\n",
    "            gesture_texts.append((\n",
    "                hand[0].category_name,\n",
    "                gesture[0].category_name,\n",
    "                gesture[0].score\n",
    "            ))\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result\n",
    ")\n",
    "\n",
    "try:\n",
    "    with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        timestamp = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process hands\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(\n",
    "                        frame, landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_draw.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                        mp_draw.DrawingSpec(color=(250, 44, 250), thickness=2)\n",
    "                    )\n",
    "            \n",
    "            # Process gestures\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "            timestamp += 1\n",
    "            recognizer.recognize_async(mp_image, timestamp)\n",
    "\n",
    "            # Draw gesture labels\n",
    "            for idx, (hand, gesture, score) in enumerate(gesture_texts):\n",
    "                cv2.putText(frame, f\"{gesture} ({score:.2f})\", \n",
    "                           (10, 30 + idx * 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                           (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('Hand Gesture Recognition', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    if 'cap' in locals():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7)\n",
    "\n",
    "# Gesture recognizer setup\n",
    "model_path = 'gesture_recognizer.task'\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Global variables\n",
    "gesture_texts = []\n",
    "volume_mode_active = False\n",
    "volume_toggle_ready = True  \n",
    "\n",
    "def print_result(result, output_image, timestamp_ms):\n",
    "    global gesture_texts\n",
    "    gesture_texts = []\n",
    "    if result.gestures and result.handedness:\n",
    "        for hand, gesture in zip(result.handedness, result.gestures):\n",
    "            gesture_texts.append((\n",
    "                hand[0].category_name,\n",
    "                gesture[0].category_name,\n",
    "                gesture[0].score\n",
    "            ))\n",
    "\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt(\n",
    "        (landmark1.x - landmark2.x) ** 2 +\n",
    "        (landmark1.y - landmark2.y) ** 2 +\n",
    "        (landmark1.z - landmark2.z) ** 2\n",
    "    )\n",
    "\n",
    "def adjust_volume(change):\n",
    "    if change > 0:\n",
    "        os.system(\"osascript -e 'set volume output volume (output volume of (get volume settings) + 5)'\")\n",
    "    elif change < 0:\n",
    "        os.system(\"osascript -e 'set volume output volume (output volume of (get volume settings) - 5)'\")\n",
    "\n",
    "# Default distance threshold\n",
    "default_distance = None\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result\n",
    ")\n",
    "\n",
    "try:\n",
    "    with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        timestamp = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process hands\n",
    "            hand_results = hands.process(frame_rgb)\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(\n",
    "                        frame, landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_draw.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                        mp_draw.DrawingSpec(color=(250, 44, 250), thickness=2)\n",
    "                    )\n",
    "\n",
    "                    # Detect middle finger and thumb pinch gesture to toggle volume mode\n",
    "                    thumb_tip = landmarks.landmark[4]\n",
    "                    middle_tip = landmarks.landmark[12]\n",
    "                    pinch_distance = calculate_distance(thumb_tip, middle_tip)\n",
    "\n",
    "                    if pinch_distance < 0.05 and volume_toggle_ready:  # Threshold for pinch gesture\n",
    "                        volume_mode_active = not volume_mode_active\n",
    "                        mode_status = \"ON\" if volume_mode_active else \"OFF\"\n",
    "                        volume_toggle_ready = False  # Prevent multiple toggles in one pinch\n",
    "                        cv2.putText(frame, f\"Volume Mode {mode_status}\", \n",
    "                                   (50, 100),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                                   (255, 255, 0), 2, cv2.LINE_AA)\n",
    "                        print(f\"Volume Mode {mode_status}\")\n",
    "\n",
    "                    if pinch_distance > 0.1:  # Reset toggle readiness when fingers move apart\n",
    "                        volume_toggle_ready = True\n",
    "\n",
    "                    # If volume mode is active, process thumb and index finger gestures for volume control\n",
    "                    if volume_mode_active:\n",
    "                        index_tip = landmarks.landmark[8]\n",
    "                        thumb_tip = landmarks.landmark[4]\n",
    "                        distance = calculate_distance(thumb_tip, index_tip)\n",
    "\n",
    "                        if default_distance is None:\n",
    "                            default_distance = distance  # Set the initial distance as default\n",
    "                            print(f\"Default distance set: {default_distance}\")\n",
    "                        else:\n",
    "                            if distance < default_distance * 0.8:  # Threshold for volume up\n",
    "                                cv2.putText(frame, \"Volume Up!\", \n",
    "                                           (50, 50),\n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                                           (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                                adjust_volume(1)\n",
    "                                print(\"Volume Up Gesture Detected\")\n",
    "                            elif distance > default_distance * 1.2:  # Threshold for volume down\n",
    "                                cv2.putText(frame, \"Volume Down!\", \n",
    "                                           (50, 50),\n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                                           (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                                adjust_volume(-1)\n",
    "                                print(\"Volume Down Gesture Detected\")\n",
    "\n",
    "            # Process gestures\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "            timestamp += 1\n",
    "            recognizer.recognize_async(mp_image, timestamp)\n",
    "\n",
    "            # Draw gesture labels\n",
    "            for idx, (hand, gesture, score) in enumerate(gesture_texts):\n",
    "                cv2.putText(frame, f\"{gesture} ({score:.2f})\", \n",
    "                           (10, 30 + idx * 30),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, \n",
    "                           (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('Hand Gesture Recognition', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    if 'cap' in locals():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
